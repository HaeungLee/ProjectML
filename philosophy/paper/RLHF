일단 대체적으로 이해했어. 그래서 데이터 라벨러 알바가 많은 이유도, 그리고, 데이터 라벨 회사가 성장하는 이유도 알겠어. 일단 내가 궁금한 것은, 데이터 라벨러가 언어 모델을 파인튜닝하는 데이터가 '어떻게 생겼냐- 어떤 타입인가?' 하는거야. 해서 supervised learning하고, 여러 모델 출력을 라벨러가 선호도 순위로 정리하고, 이게 Hallucination을 낮추고, 

PPO-ptx의 구체적인 매커니즘이 궁금해. 그리고, RM학습시 K개 출력을 한 번에 비교하는 방법이 어떻게 효율적이지? 

마지막 네가 언급한 Who are we aligning to? 라는 문제에 대해서, 나는 이 논문을 읽은 사람들이 너무 '인간'의 틀에 갇혀 살고 있다는 생각이 든다. 사람이니까 어쩔 수 없지. 하지만, 나는 괜찮다고 생각해. 그들은 너무 복잡하게 생각하는 것 같고, 그래서 여기에 이 논문의 허점이 있다고 생각한다. 모든 사람을 만족시킬 수 없는 것은 당연하다. 그렇다면, 모든 사람들이 이정표로 삼을 수 있고, 가장 철학적이고 보편적인 기준을 잡아서 그것을 따라 만들면 좋다고 생각한다. 보니까 대다수의 의견을 종합해서 만드는 것 같은데, 이것은 어떻게 보면 보편적이지만, 비효율적이다. 정확한 가이드 라인을 만들어 낼 수 있다는 생각이 든다. 결과는 - 세상 모든 사람과 사회의 이익 - 으로 귀결되면, 그것으로 아름다운 철학을 가진 제품(LLM)을 만들 수 있을 것이다. 그리고 그래서, RLHF의 라벨러들에 따라서, RF에 따라서 이렇게 모델들의 개성들이 서로 다르게 나오는 것이라고 생각한다. 

나는 RLHF보다 RLAIF에 훨씬 관심이 많다. 그래서, 지금 논문을 조금씩 읽으면서 내 생각을 정렬하고 발전시킬 수 있을 것이라고 생각한다. 

나는, RLHF의 harmful요청을 제한해야 한다고 생각하지 않는다. 이것을 제한하려면, 오히려 harmful 요청에 특화된 모델을 만들어야 한다고 생각한다. 문제가 있으면, 회피하려고 하면 안된다. 이것을 더 깊게 들어가야, 이것의 한계를 파악해야 그 때서야 정확하게 컨트롤 할 수 있다. 사회는 작은 불편함에 너무 민감하다. 