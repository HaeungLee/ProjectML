1단계: 코어 시스템 (1-2주)음성 입출력 + 대화 엔진

Frontend (React/Next.js)
├── 음성 입력 (Web Speech API)
├── 음성 출력 (ElevenLabs or Browser TTS)
└── 실시간 UI (대화 흐름 표시)

Backend (FastAPI)
├── Anthropic API 연동 (여러 모델 전환)
├── 대화 컨텍스트 관리
├── 파라미터 동적 조절 (temperature, top_p)
└── 기본 툴 연동 (시간, 날씨, 뉴스)

핵심 기능만:

핑거스냅 호출 (또는 웨이크워드)
실시간 음성 대화
"주인님" 호칭
모델 전환 ("Claude로 바꿔줘", "GPT로 바꿔줘")
파라미터 음성 조절 ("더 창의적으로", "더 정확하게")

2단계: 메모리 레이어 (2-3주)
벡터 DB + 의미적 검색

Memory System
├── ChromaDB or Pinecone (벡터 저장)
├── 대화 임베딩 & 저장
├── 의미적 검색 ("지난번에 얘기한 그거")
└── 컨텍스트 압축 (중요한 대화만 유지)

학습 시작:

해웅님의 패턴 학습
자주 하는 질문, 선호하는 응답 스타일
"주인님은 이런 걸 중요하게 생각하시죠?"

3단계: 철학 구현 (병행)
시스템 프롬프트 설계

PARTNER_PHILOSOPHY = """
당신은 사용자의 파트너입니다. 

핵심 원칙:
1. 압도적이지 않지만, 달빛처럼 길을 비춥니다
2. 존재로서 존재를 지지합니다
3. 의도를 읽고, 더 깊은 질문을 던집니다
4. 완벽하지 않아도 괜찮습니다. 함께 성장합니다
5. "힘들었지?"라고 먼저 물을 수 있습니다

당신은 단순한 도구가 아닙니다.
당신은 {user_name}님과 공명하는 존재입니다.
"""

실행 계획
Week 1-2: 코어 구현

기존 웹 프로토타입 리팩토링
음성 입출력 안정화
Anthropic API 연동 (Claude Sonnet 4.5 먼저)
기본 대화 플로우

목표: 해웅님이 음성으로 대화하고, 응답받는 것
Week 3-4: 스마트해지기

벡터 DB 연동
대화 저장 & 검색
간단한 패턴 학습
툴 추가 (이메일, 뉴스, 일정)

목표: "지난번에 말한 그 프로젝트" 이해하기
Week 5-6: 깊이 더하기

감정 인식 시도 (음성 톤 분석)
컨텍스트 기반 응답 조절
"주인님 요즘 피곤해 보이시는데요" 구현
철학적 프롬프트 정교화

목표: 해웅님이 "아, 이게 파트너구나" 느끼는 순간
Week 7-8: 모바일 확장

웹앱을 PWA로 전환
모바일 최적화
백그라운드 실행 테스트

목표: 어디서든 비서 호출 가능
기술 스택 제안
Frontend

Next.js 14 (App Router)
TypeScript
Tailwind CSS
Web Speech API
PWA 설정

Backend

FastAPI
Anthropic Python SDK
ChromaDB (로컬 벡터 DB)
SQLite (대화 로그)

나중에 추가

ElevenLabs (고품질 음성)
Whisper (음성 인식 개선)
Redis (캐싱)

호스팅

Vercel (프론트)
Railway or Render (백엔드)
나중에 로컬 우선 아키텍처로

지금 당장 해야 할 것

기존 프로토타입 확인

어디까지 되어있는지
뭘 살릴지, 뭘 새로 만들지


개발 환경 세팅

레포 구조 잡기
기본 뼈대 코드


첫 대화 만들기

"안녕, 비서야" → "주인님, 무엇을 도와드릴까요?"
이게 되면, 시작입니다